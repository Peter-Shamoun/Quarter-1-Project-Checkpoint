program: train.py
method: bayes
metric:
  name: eval_perplexity_mean
  goal: minimize

# Early termination to save compute on poorly performing runs
early_terminate:
  type: hyperband
  min_iter: 3
  eta: 2

parameters:
  # Required experiment identifiers
  experiment.name:
    value: sweep-run
  
  experiment.group:
    value: babylm-sweep
  
  # Learning rate - most critical hyperparameter for transformer training
  trainer.lr:
    distribution: log_uniform_values
    min: 1e-4
    max: 2e-3
  
  # Batch size - affects training dynamics and memory
  trainer.batch_size:
    values: [16, 32, 64]
  
  # Warmup steps - important for training stability
  trainer.num_warmup_steps:
    values: [10000, 25000, 40000]
  
  # Reduce training steps for faster sweep iteration (50k for 5hr budget)
  trainer.max_training_steps:
    value: 50000
  
  # Keep other settings consistent
  experiment.seed:
    value: 42
  
  # Enable downstream evaluation
  trainer.eval_blimp:
    value: true
  
  trainer.eval_glue:
    value: true

# Run name will be automatically generated by WandB sweep
command:
  - ${env}
  - python
  - ${program}
  - ${args_no_hyphens}

